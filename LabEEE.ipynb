{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3husK_hSdNe"
   },
   "source": [
    "C:\\Users\\michi\\Desktop\\01. Outdoor-Indoor\\01. LabEEE Experiments\\06. LabEEE A2 20240123\\00 DATA\\ZM - Zac Milioli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04PmjXwf-Rwq"
   },
   "source": [
    "# Bibliotecas usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "Q2tcc8yH-FpN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pauRK7Fl-Ybr"
   },
   "source": [
    "# Importar bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAAhzRi7QHyS",
    "outputId": "26a198cd-5c10-417e-aad7-7930d5c9427e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n"
     ]
    }
   ],
   "source": [
    "# localizar todos os arquivos csv dos experimentos\n",
    "padrao_busca = \"/Users/michi/Desktop/01. Outdoor-Indoor/01. LabEEE Experiments/**/*.csv\"\n",
    "todos_arquivos = glob.glob(padrao_busca, recursive=True)\n",
    "\n",
    "# filtra os arquivos de interesse\n",
    "ids = [\"IS\", \"LG\", \"LM\", \"MF\", \"TM\", \"ZM\", \"AW\", \"JF\", \"JP\", \"MN\", \"NC\", \"RM\"]\n",
    "nao = [\"01. Pilot LabEEE 20231114\", \"02. Pilot LabEEE 20231122\"]\n",
    "arquivos_csv = [arquivo for arquivo in todos_arquivos if any(sigla in arquivo for sigla in ids) and not any(nome in arquivo for nome in nao)]\n",
    "\n",
    "print(len(arquivos_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "relacao_tabela_informacao = {\"A\" :\t[\"4B00000069A8E941\",\"FE00000069AF1041\",\"B600000069B23241\",\"7D00000069AB4641\",\"0F00000069AAAF41\",\"C400000069ADCC41\"],\n",
    "                            \"D\":\t[\"CB00000069AD3041\",\"9400000069AA4941\",\"2700000069AD8541\",\"C100000069B26141\",\"CA00000069B19041\",\"4A00000069AC0841\"],\n",
    "                            \"F\":\t[\"C400000069AA8341\",\"C400000069B3DB41\",\"8100000069AE1641\",\"2700000069AACA41\",\"6700000069AEFC41\"\t\"1C00000069B3F941\"], \n",
    "                            \"H\":\t[\"B200000069AE3141\",\"EC00000069A9CD41\",\"3300000069AC2241\",\"3600000069AAD741\",\"E800000069AAC041\",\"AD00000069AF4C41\"],\n",
    "                            \"J\":\t[\"FA00000069AD0441\",\"3400000069B1AE41\",\"C700000069B34D41\",\"4B00000069B0A841\",\"0600000069B22741\",\"9100000069AAEA41\",\"6700000069AB8141\"],\n",
    "                            \"K\":\t[\"FB00000069B28F41\",\"7B00000069AA6A41\",\"E500000069AA2F41\",\"1A00000069A9BF41\",\"FE00000069B10741\",\"E100000069AD4641\"],\n",
    "                            \"M\":\t[\"1B00000069B06241\",\"8C00000069ADD241\",\"F000000069AF6941\",\"AA00000069AA8141\",\"1600000069ABE741\",\"A000000069AEBA41\"],\n",
    "                            \"O\":\t[\"7600000069AA8541\",\"F200000069B16D41\",\"3000000069A9C941\",\"BF00000069AFC741\",\"A300000069B12241\",\"D900000069ACA241\"],\n",
    "                            \"Q\":\t[\"8600000069B0B141\",\"6000000069AB3141\",\"DB00000069A9CC41\",\"EF00000069B23141\",\"6900000069AE8541\",\"4300000069B3CF41\"],\n",
    "                            \"T\":\t[\"7600000069AB9C41\",\"4500000069B1C841\",\"6600000069B25C41\",\"A200000069B0BE41\",\"D700000069B3D541\",\"9800000069B26241\",\"9E00000069AE7241\"],\n",
    "                            \"T_RH\" : [\"6E0000006CB57E41\",\t\"590000006CB46641\",\t\"4B0000006CBA7341\",\t\"D30000006CBD4C41\",\t\"370000006341C241\",\t\"9C000000633F9F41\"]}\n",
    "\n",
    "\n",
    "nomes = [\"Zac\", \"Joao\", \"Igor\", \"Luis\", \"Milena\", \"Ana\", \"Marcela\", \"Nathalia\", \"Thalita\", \"Liege\", \"Rayner\", \"João\"]\n",
    "sports = [\"WALKING\",\"CYCLING\",\"CROSS-COUNTRY_SKIING\",\"SKATING\",\"RIDING\",\"ROWING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifica_qual_tipo_de_csv(titulo):\n",
    "    for chave, valores in relacao_tabela_informacao.items():\n",
    "        if titulo in valores:\n",
    "            return chave\n",
    "\n",
    "def convertepfloat(row):\n",
    "    n1 = str(row.iloc[2]).replace('.', '')\n",
    "    n2 = str(row.iloc[3]).replace('.', '')\n",
    "    return float(n1 + '.' + n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cardiacos = []\\ntemperaturas = []\\nfor arquivo in arquivos_csv:\\n    ultimo_backslash = arquivo.rfind(\\'\\\\\\')\\n    ultimounderscore = arquivo.rfind(\\'_\\')\\n    titulo = arquivo[ultimo_backslash+1:ultimounderscore]\\n    if \"Test+Room\" in titulo or any(name in titulo for name in nomes):\\n        cardiacos.append(arquivo)\\n    elif(\"_U\" in arquivo) or (\"_T\" in arquivo) or (any(titulo in valores for valores in relacao_tabela_informacao.values())):\\n        temperaturas.append(arquivo)\\n    else:\\n        x = input(f\"isso é cardiaco (s): {titulo}\")\\n        if (x == \"s\"):\\n            cardiacos.append(arquivo)\\n        else:\\n            temperaturas.append(arquivo)\\n        \\n\\n\\nprint(len(cardiacos)+len(temperaturas))'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''cardiacos = []\n",
    "temperaturas = []\n",
    "for arquivo in arquivos_csv:\n",
    "    ultimo_backslash = arquivo.rfind('\\\\')\n",
    "    ultimounderscore = arquivo.rfind('_')\n",
    "    titulo = arquivo[ultimo_backslash+1:ultimounderscore]\n",
    "    if \"Test+Room\" in titulo or any(name in titulo for name in nomes):\n",
    "        cardiacos.append(arquivo)\n",
    "    elif(\"_U\" in arquivo) or (\"_T\" in arquivo) or (any(titulo in valores for valores in relacao_tabela_informacao.values())):\n",
    "        temperaturas.append(arquivo)\n",
    "    else:\n",
    "        x = input(f\"isso é cardiaco (s): {titulo}\")\n",
    "        if (x == \"s\"):\n",
    "            cardiacos.append(arquivo)\n",
    "        else:\n",
    "            temperaturas.append(arquivo)\n",
    "        \n",
    "\n",
    "\n",
    "print(len(cardiacos)+len(temperaturas))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FY756p1DAJs9",
    "outputId": "b8de568b-1fd3-47b4-b41c-122f53345981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Date, id, HR (bpm) W, HR (bpm) C]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Cria um DataFrame para todos\n",
    "col_names = [\"Date\",\"id\",'HR (bpm) W','HR (bpm) C']\n",
    "BaseDeDados = pd.DataFrame(columns=col_names)\n",
    "\n",
    "\"\"\"for i, id in enumerate(ids):\n",
    "  substituivel = pd.DataFrame(columns=col_names)\n",
    "  inicio = '2024-01-16 07:00:00'\n",
    "  fim = '2024-02-07 19:00:00'\n",
    "  frequencia = 's'\n",
    "  intervalos_tempo = pd.date_range(start=inicio, end=fim, freq=frequencia)\n",
    "  substituivel['Date'] = intervalos_tempo\n",
    "  substituivel['iD'] = id\n",
    "  BaseDeDados = pd.concat([BaseDeDados, substituivel], ignore_index=True)\"\"\"\n",
    "\n",
    "print(BaseDeDados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6700000069AEFC41\n",
      "1C00000069B3F941\n",
      "1C00000069B3F941\n",
      "6700000069AEFC41\n",
      "1C00000069B3F941\n",
      "6700000069AEFC41\n",
      "6700000069AEFC41\n",
      "1C00000069B3F941\n",
      "6700000069AEFC41\n",
      "1C00000069B3F941\n",
      "6700000069AEFC41\n",
      "6700000069AEFC41\n"
     ]
    }
   ],
   "source": [
    "# criar funções para ações repetidas\n",
    "# substituir if/elif/else por match case\n",
    "\n",
    "for arquivo in arquivos_csv:\n",
    "    ultimo_backslash = arquivo.rfind('\\\\')\n",
    "    ultimounderscore = arquivo.rfind('_')\n",
    "    titulo = arquivo[ultimo_backslash+1:ultimounderscore]\n",
    "    penultimo_backslash = arquivo.rfind('\\\\', 0, ultimo_backslash)\n",
    "    id = arquivo[penultimo_backslash+1:penultimo_backslash+3]\n",
    "    if \"Test+Room\" in titulo or any(name in titulo for name in nomes):\n",
    "        heart = pd.read_csv(arquivo).fillna('0')\n",
    "        if(heart.iloc[0,1] == \"OTHER_OUTDOOR\"):\n",
    "            Dia = heart.iloc[0,2]\n",
    "            HInicio = heart.iloc[0,3]\n",
    "            DataInicio = Dia + \" \" + HInicio\n",
    "            pulso = pd.read_csv(arquivo, usecols=[\"Time\",\"HR (bpm)\"], skiprows=2)\n",
    "            pulso['id'] = id\n",
    "            pulso = pulso.rename(columns={'Time': 'Date'})\n",
    "            pulso = pulso.rename(columns={'HR (bpm)': 'HR (bpm) W'})\n",
    "            # arrumar a data\n",
    "            BaseDeDados = pd.concat([BaseDeDados, pulso], ignore_index=True)\n",
    "        elif(heart.iloc[0,1] in sports):\n",
    "            Dia = heart.iloc[0,2]\n",
    "            HInicio = heart.iloc[0,3]\n",
    "            DataInicio = Dia + \" \" + HInicio\n",
    "            peito = pd.read_csv(arquivo, usecols=[\"Time\",\"HR (bpm)\"], skiprows=2)\n",
    "            peito['id'] = id\n",
    "            peito = peito.rename(columns={'Time': 'Date'})\n",
    "            peito = peito.rename(columns={'HR (bpm)': 'HR (bpm) C'})\n",
    "            # arrumar a data\n",
    "            BaseDeDados = pd.concat([BaseDeDados, peito], ignore_index=True)\n",
    "        elif(heart.iloc[0,1] == \"False\" or \"True\"):\n",
    "            continue\n",
    "        else:\n",
    "            print(heart.iloc[0,1])    \n",
    "    elif(\"_U\" in arquivo) or (\"_T\" in arquivo) or (any(titulo in valores for valores in relacao_tabela_informacao.values())):\n",
    "        colunas = [\"Date\", \"uni\", \"u\", \"d\"]\n",
    "        if(\"_T\" in arquivo):\n",
    "            # somar coluna de unidade de medica e valor\n",
    "            temp = pd.read_csv(arquivo, skiprows=20,names=colunas).fillna('0')\n",
    "            temp['id'] = id\n",
    "            temp['Temp'] = temp.apply(lambda row: convertepfloat(row), axis=1)\n",
    "            temp = temp.iloc[:, [0,1,-2,-1]]\n",
    "            temp['Date'] = pd.to_datetime(temp['Date'], format='%d/%m/%y %H:%M:%S')\n",
    "            BaseDeDados = pd.concat([BaseDeDados, temp],ignore_index=True)\n",
    "        elif(\"_U\" in arquivo):\n",
    "            # somar coluna de unidade de medica e valor\n",
    "            umid = pd.read_csv(arquivo, skiprows=20,names=colunas).fillna('0')\n",
    "            umid['id'] = id\n",
    "            umid['Umid'] = umid.apply(lambda row: convertepfloat(row), axis=1)\n",
    "            umid = umid.iloc[:, [0,1,-2,-1]]\n",
    "            umid['Date'] = pd.to_datetime(umid['Date'], format='%d/%m/%y %H:%M:%S')\n",
    "            BaseDeDados = pd.concat([BaseDeDados, umid],ignore_index=True)\n",
    "        else:\n",
    "            # somar coluna de unidade de medica e valor\n",
    "            # identificar qual sensor de pele é o arquivo\n",
    "            coluna = verifica_qual_tipo_de_csv(titulo)\n",
    "            temp = pd.read_csv(arquivo, skiprows=20,names=colunas).fillna('0')\n",
    "            temp[f'{coluna}'] = temp.apply(lambda row: convertepfloat(row), axis=1)\n",
    "            temp['id'] = id\n",
    "            temp = temp.iloc[:, [0,1,-2,-1]]\n",
    "            temp['Date'] = pd.to_datetime(temp['Date'], format='%d/%m/%y %H:%M:%S')\n",
    "            BaseDeDados = pd.concat([BaseDeDados, temp],ignore_index=True)\n",
    "    else:\n",
    "        print(titulo)\n",
    "    #print(\"match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>id</th>\n",
       "      <th>HR (bpm) W</th>\n",
       "      <th>HR (bpm) C</th>\n",
       "      <th>uni</th>\n",
       "      <th>A</th>\n",
       "      <th>M</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Umid</th>\n",
       "      <th>Q</th>\n",
       "      <th>J</th>\n",
       "      <th>O</th>\n",
       "      <th>D</th>\n",
       "      <th>T</th>\n",
       "      <th>H</th>\n",
       "      <th>K</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>IS</td>\n",
       "      <td>106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:00:01</td>\n",
       "      <td>IS</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:00:02</td>\n",
       "      <td>IS</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:00:03</td>\n",
       "      <td>IS</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:00:04</td>\n",
       "      <td>IS</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date  id HR (bpm) W HR (bpm) C  uni   A   M  Temp  Umid   Q   J   O  \\\n",
       "0  00:00:00  IS        106        NaN  NaN NaN NaN   NaN   NaN NaN NaN NaN   \n",
       "1  00:00:01  IS        107        NaN  NaN NaN NaN   NaN   NaN NaN NaN NaN   \n",
       "2  00:00:02  IS        107        NaN  NaN NaN NaN   NaN   NaN NaN NaN NaN   \n",
       "3  00:00:03  IS        107        NaN  NaN NaN NaN   NaN   NaN NaN NaN NaN   \n",
       "4  00:00:04  IS        107        NaN  NaN NaN NaN   NaN   NaN NaN NaN NaN   \n",
       "\n",
       "    D   T   H   K   F  \n",
       "0 NaN NaN NaN NaN NaN  \n",
       "1 NaN NaN NaN NaN NaN  \n",
       "2 NaN NaN NaN NaN NaN  \n",
       "3 NaN NaN NaN NaN NaN  \n",
       "4 NaN NaN NaN NaN NaN  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaseDeDados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmesclada = BaseDeDados['HR (bpm)_x'].combine_first(BaseDeDados['HR (bpm)_y']).combine_first(BaseDeDados['HR (bpm)'])\\nBaseDeDados.drop(columns=['HR (bpm)_x','HR (bpm)_y','HR (bpm)'], inplace=True)\\nBaseDeDados['HR (bpm)'] = mesclada\\nBaseDeDados.head(300)\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "mesclada = BaseDeDados['HR (bpm)_x'].combine_first(BaseDeDados['HR (bpm)_y']).combine_first(BaseDeDados['HR (bpm)'])\n",
    "BaseDeDados.drop(columns=['HR (bpm)_x','HR (bpm)_y','HR (bpm)'], inplace=True)\n",
    "BaseDeDados['HR (bpm)'] = mesclada\n",
    "BaseDeDados.head(300)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = [\"Date\", \"Unit\", \"Value\", \"Decimal\"]\n",
    "\n",
    "def verifica_qual_tipo_de_csv(titulo):\n",
    "    for chave, valores in relacao_tabela_informacao.items():\n",
    "        if titulo in valores:\n",
    "            return chave\n",
    "\n",
    "def convertepfloat(row):\n",
    "    n1 = str(row[2]).replace('.', '')\n",
    "    n2 = str(row[3]).replace('.', '')\n",
    "    return float(n1 + '.' + n2)\n",
    "\n",
    "for arquivo in arquivos_IS:\n",
    "    ultimo_backslash = arquivo.rfind('\\\\')\n",
    "    ultimounderscore = arquivo.rfind('_')\n",
    "    nomes = [\"Zac\", \"Joao\", \"Igor\", \"Luis\", \"Milena\", \"Ana\", \"Marcela\", \"Nathalia\", \"Thalita\", \"Liege\", \"Rayner\", \"João\"]\n",
    "    titulo = arquivo[ultimo_backslash+1:ultimounderscore]\n",
    "    ehhr = any(nome in titulo for nome in nomes)\n",
    "    \n",
    "    if(any(titulo in valores for valores in relacao_tabela_informacao.values())):\n",
    "        coluna = verifica_qual_tipo_de_csv(titulo)\n",
    "        temp = pd.read_csv(arquivo, names=colNames, skiprows=20,).fillna('0')\n",
    "        temp['id'] = \"IS\"\n",
    "        print(temp)\n",
    "        temp[f\"{coluna}\"] = temp.apply(lambda row: convertepfloat(row), axis=1)\n",
    "        temp = temp.iloc[:, [0,-2,-1]]\n",
    "        temp['Date'] = pd.to_datetime(temp['Date'], format='%d/%m/%y %H:%M:%S')\n",
    "        BaseDeDados = pd.concat([BaseDeDados, temp],ignore_index=True)\n",
    "        print(BaseDeDados)\n",
    "        #BaseDeDados = BaseDeDados.dropna(axis=1, how='all')\n",
    "    #elif(ehhr):\n",
    "        #continue\n",
    "    #elif(\"Test+Room\" in arquivo):\n",
    "        #continue\n",
    "    #elif(arquivo[-5] in (\"T\", \"U\")):\n",
    "        #continue\n",
    "    #else:\n",
    "        #print(titulo, \"* não foi *\", arquivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>id</th>\n",
       "      <th>HR (bpm) W</th>\n",
       "      <th>HR (bpm) C</th>\n",
       "      <th>uni</th>\n",
       "      <th>A</th>\n",
       "      <th>M</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Umid</th>\n",
       "      <th>Q</th>\n",
       "      <th>J</th>\n",
       "      <th>O</th>\n",
       "      <th>D</th>\n",
       "      <th>T</th>\n",
       "      <th>H</th>\n",
       "      <th>K</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>IS</td>\n",
       "      <td>106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:00:01</td>\n",
       "      <td>IS</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:00:02</td>\n",
       "      <td>IS</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:00:03</td>\n",
       "      <td>IS</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:00:04</td>\n",
       "      <td>IS</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652821</th>\n",
       "      <td>2024-01-30 15:48:01</td>\n",
       "      <td>IS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.947</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652822</th>\n",
       "      <td>2024-01-30 15:49:01</td>\n",
       "      <td>IS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.822</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652823</th>\n",
       "      <td>2024-01-30 15:50:01</td>\n",
       "      <td>IS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.760</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652824</th>\n",
       "      <td>2024-01-30 15:51:01</td>\n",
       "      <td>IS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.510</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652825</th>\n",
       "      <td>2024-01-30 15:52:01</td>\n",
       "      <td>IS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.385</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1652826 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date  id HR (bpm) W HR (bpm) C  uni   A   M  Temp  \\\n",
       "0                   00:00:00  IS        106        NaN  NaN NaN NaN   NaN   \n",
       "1                   00:00:01  IS        107        NaN  NaN NaN NaN   NaN   \n",
       "2                   00:00:02  IS        107        NaN  NaN NaN NaN   NaN   \n",
       "3                   00:00:03  IS        107        NaN  NaN NaN NaN   NaN   \n",
       "4                   00:00:04  IS        107        NaN  NaN NaN NaN   NaN   \n",
       "...                      ...  ..        ...        ...  ...  ..  ..   ...   \n",
       "1652821  2024-01-30 15:48:01  IS        NaN        NaN  NaN NaN NaN   NaN   \n",
       "1652822  2024-01-30 15:49:01  IS        NaN        NaN  NaN NaN NaN   NaN   \n",
       "1652823  2024-01-30 15:50:01  IS        NaN        NaN  NaN NaN NaN   NaN   \n",
       "1652824  2024-01-30 15:51:01  IS        NaN        NaN  NaN NaN NaN   NaN   \n",
       "1652825  2024-01-30 15:52:01  IS        NaN        NaN  NaN NaN NaN   NaN   \n",
       "\n",
       "         Umid   Q   J   O   D   T   H       K   F  \n",
       "0         NaN NaN NaN NaN NaN NaN NaN     NaN NaN  \n",
       "1         NaN NaN NaN NaN NaN NaN NaN     NaN NaN  \n",
       "2         NaN NaN NaN NaN NaN NaN NaN     NaN NaN  \n",
       "3         NaN NaN NaN NaN NaN NaN NaN     NaN NaN  \n",
       "4         NaN NaN NaN NaN NaN NaN NaN     NaN NaN  \n",
       "...       ...  ..  ..  ..  ..  ..  ..     ...  ..  \n",
       "1652821   NaN NaN NaN NaN NaN NaN NaN  26.947 NaN  \n",
       "1652822   NaN NaN NaN NaN NaN NaN NaN  26.822 NaN  \n",
       "1652823   NaN NaN NaN NaN NaN NaN NaN  26.760 NaN  \n",
       "1652824   NaN NaN NaN NaN NaN NaN NaN  26.510 NaN  \n",
       "1652825   NaN NaN NaN NaN NaN NaN NaN  26.385 NaN  \n",
       "\n",
       "[1652826 rows x 17 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaseDeDados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemas para amanhã:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michi\\AppData\\Local\\Temp\\ipykernel_6172\\921341138.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  n1 = str(row[2]).replace('.', '')\n",
      "C:\\Users\\michi\\AppData\\Local\\Temp\\ipykernel_6172\\921341138.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  n2 = str(row[3]).replace('.', '')\n"
     ]
    },
    {
     "ename": "DateParseError",
     "evalue": "day is out of range for month: 0, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mparsing.pyx:684\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: day is out of range for month",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDateParseError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m a[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoluna\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: convertepfloat(row), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39miloc[:, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m---> 13\u001b[0m a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m basededados \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(basededados, a, on \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m basededados \u001b[38;5;241m=\u001b[39m basededados\u001b[38;5;241m.\u001b[39mfilter(regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^(?!.*_(x|y)$).*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\tools\\datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[1;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\tools\\datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[1;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\tools\\datetimes.py:435\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[1;32m--> 435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mobjects_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     out_unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arrays\\datetimes.py:2398\u001b[0m, in \u001b[0;36mobjects_to_datetime64\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[0m\n\u001b[0;32m   2395\u001b[0m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mobject_)\n\u001b[1;32m-> 2398\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2403\u001b[0m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreso\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mabbrev_to_npy_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2408\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m   2409\u001b[0m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[0;32m   2410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "File \u001b[1;32mtslib.pyx:414\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtslib.pyx:596\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtslib.pyx:553\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mconversion.pyx:641\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsing.pyx:336\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsing.pyx:688\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDateParseError\u001b[0m: day is out of range for month: 0, at position 0"
     ]
    }
   ],
   "source": [
    "def convertepfloat(row):\n",
    "  n1 = str(row[2]).replace('.', '')\n",
    "  n2 = str(row[3]).replace('.', '')\n",
    "  return float(n1 + '.' + n2)\n",
    "\n",
    "for arquivo in arquivos_csv:\n",
    "  coluna = verifica_qual_tipo_de_csv(arquivo)\n",
    "  colNames = [\"Date\", \"Unit\", \"Value\", \"Decimal\"]\n",
    "\n",
    "  a = pd.read_csv(arquivo, names=colNames , skiprows=20,).fillna('0')\n",
    "  a[f\"{coluna}\"] = a.apply(lambda row: convertepfloat(row), axis=1)\n",
    "  a = a.iloc[:, [0, -1]]\n",
    "  a['Date'] = pd.to_datetime(a['Date'])\n",
    "  basededados = pd.merge(basededados, a, on = 'Date', how='outer')\n",
    "  basededados = basededados.filter(regex='^(?!.*_(x|y)$).*')\n",
    "  basededados = basededados.dropna(axis=1, how='all')\n",
    "\n",
    "print(basededados.iloc[[1, 61, 121, 181]])\n",
    "basededados.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-7jK1g9jiSE",
    "outputId": "a7625e11-8aa4-4698-b8e1-4fe3bb104237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "53\n",
      "0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'IS - Igor Schlichting\\\\05_Igor_2024-01-16_08-13-26.CSV'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m coluna \u001b[38;5;241m=\u001b[39m verifica_qual_tipo_de_csv(arquivo)\n\u001b[0;32m      5\u001b[0m colNames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecimal\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43marquivo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolNames\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m a[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoluna\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: unireconverter(row), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39miloc[:, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'IS - Igor Schlichting\\\\05_Igor_2024-01-16_08-13-26.CSV'"
     ]
    }
   ],
   "source": [
    "\n",
    "for arquivo in arquivos_csv:\n",
    "  print(len(\"IS - Igor Schlichting\\\\\"))\n",
    "  print(len(arquivo))\n",
    "  coluna = verifica_qual_tipo_de_csv(arquivo)\n",
    "  colNames = [\"Date\", \"Unit\", \"Value\", \"Decimal\"]\n",
    "  a = pd.read_csv(arquivo, names=colNames , skiprows=20,).fillna('0')\n",
    "  a[f\"{coluna}\"] = a.apply(lambda row: unireconverter(row), axis=1)\n",
    "  a = a.iloc[:, [0, -1]]\n",
    "  a['Date'] = pd.to_datetime(a['Date'])\n",
    "  basededados = pd.merge(basededados, a, on = 'Date', how='outer')\n",
    "  basededados = basededados.filter(regex='^(?!.*_(x|y)$).*')\n",
    "  basededados = basededados.dropna(axis=1, how='all')\n",
    "\n",
    "print(basededados.iloc[[1, 61, 121, 181]])\n",
    "basededados.shape\n",
    "#basededados = basededados.iloc[:, list(range(2)) + list(range(-12, 0))]\n",
    "\n",
    "#nome_do_arquivo = 'meu_arquivo_excel.xlsx'  # Nome do arquivo Excel\n",
    "#basededados.to_excel(nome_do_arquivo, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSfwY4qfSiid"
   },
   "source": [
    "# n deu certo apartir daqui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "upLMD_XCR49d",
    "outputId": "69f18ef7-c3bb-42fa-bd49-c31c5ecfcf51"
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 20, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-c03443e12c59>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 20, saw 3\n"
     ]
    }
   ],
   "source": [
    "for name in files:\n",
    "  a = pd.read_csv(namef, names=col_names)\n",
    "  df.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUdqPHsuOEcO"
   },
   "source": [
    "usar glob para colocar todos os excels em uma lista e p conferir a correção de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdJHd4vYJ5Lm"
   },
   "outputs": [],
   "source": [
    "\n",
    "all_files = glob.glob('*.xlsx')\n",
    "li = []\n",
    "for f in all_files:\n",
    "    temp_df = pd.read_csv(f, names=col_names, skiprows=20).fillna('0')\n",
    "    temp_df[\"Value\"] = temp_df[\"Value\"].astype(str)\n",
    "    temp_df[\"Value\"] = temp_df[\"Value\"].str.replace(\".0\",\"\")\n",
    "    temp_df[\"Decimal\"] = temp_df[\"Decimal\"].astype(str)\n",
    "    temp_df[\"Decimal\"] = temp_df[\"Decimal\"].str.replace(\".0\",\"\")\n",
    "    temp_df.insert(loc=4, column='Temp', value=(temp_df[\"Value\"].astype(str)+\".\"+temp_df[\"Decimal\"].astype(str)))\n",
    "    temp_df[\"Temp\"] = temp_df[\"Temp\"].astype(float)\n",
    "    li.append(temp_df)\n",
    "    print(f'Successfully created dataframe for {f} with shape {temp_df.shape}')\n",
    "    temp_df.to_csv(f)\n",
    "    print(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PdIY7Kv-FpO"
   },
   "outputs": [],
   "source": [
    "# read Tp\n",
    "Tp = glob.glob('*T.csv')\n",
    "\n",
    "# edit the filename\n",
    "T_df = pd.read_csv(''.join(Tp))#, names=col_names, skiprows=20).fillna(0)\n",
    "T_df.rename(columns={'Temp': 'wTemp'}, inplace=True)\n",
    "\n",
    "# read Up\n",
    "Up = glob.glob('*U.csv')\n",
    "\n",
    "# edit the filename\n",
    "U_df = pd.read_csv(''.join(Up))#, names=col_names, skiprows=20).fillna(0)\n",
    "U_df.rename(columns={'Temp': 'wRH'}, inplace=True)\n",
    "\n",
    "# merge T and U\n",
    "prox = pd.merge(T_df[['Date','wTemp']], U_df[['Date', 'wRH']], on='Date', how='outer')\n",
    "prox['ID']= id\n",
    "#prox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeejSnA5-FpO"
   },
   "source": [
    "### <mark> SET 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qy3KT4pt-FpQ"
   },
   "outputs": [],
   "source": [
    "# paths to the data\n",
    "pa1 = glob.glob(\"4B00000069A8E941\" + '*.csv')\n",
    "pd1 = glob.glob(\"CB00000069AD3041\" + '*.csv')\n",
    "pf1 = glob.glob(\"C400000069AA8341\" + '*.csv')\n",
    "ph1 = glob.glob(\"B200000069AE3141\" + '*.csv')\n",
    "pj1 = glob.glob(\"FA00000069AD0441\" + '*.csv')\n",
    "pk1 = glob.glob(\"FB00000069B28F41\" + '*.csv')\n",
    "pm1 = glob.glob(\"1B00000069B06241\" + '*.csv')\n",
    "po1 = glob.glob(\"7600000069AA8541\" + '*.csv')\n",
    "pq1 = glob.glob(\"8600000069B0B141\" + '*.csv')\n",
    "pt1 = glob.glob(\"7600000069AB9C41\" + '*.csv')\n",
    "\n",
    "# read all data needed\n",
    "a1 = pd.read_csv(''.join(pa1))\n",
    "d1 = pd.read_csv(''.join(pd1))\n",
    "f1 = pd.read_csv(''.join(pf1))\n",
    "h1 = pd.read_csv(''.join(ph1))\n",
    "j1 = pd.read_csv(''.join(pj1))\n",
    "k1 = pd.read_csv(''.join(pk1))\n",
    "m1 = pd.read_csv(''.join(pm1))\n",
    "o1 = pd.read_csv(''.join(po1))\n",
    "q1 = pd.read_csv(''.join(pq1))\n",
    "t1 = pd.read_csv(''.join(pt1))\n",
    "\n",
    "date = a1[\"Date\"]\n",
    "\n",
    "# create dataframe with the first (A) iButton date\n",
    "df_1 = pd.DataFrame(index=[date])\n",
    "\n",
    "# \"vlookup\" to fill the temperature data\n",
    "df_1 = pd.merge(df_1, a1[['Date', 'Temp']], on='Date', how='left')\n",
    "df_1 = df_1.rename(columns={'Temp':'A'})\n",
    "df_1 = pd.merge(df_1, d1[['Date', 'Temp']], on='Date', how='left')\n",
    "df_1 = df_1.rename(columns={'Temp':'D'})\n",
    "df_1 = pd.merge(df_1, f1[['Date', 'Temp']], on='Date', how='left')\n",
    "df_1 = df_1.rename(columns={'Temp':'F'})\n",
    "df_1 = pd.merge(df_1, h1[['Date', 'Temp']], on='Date', how='left')\n",
    "df_1 = df_1.rename(columns={'Temp':'H'})\n",
    "df_1 = pd.merge(df_1, j1[['Date', 'Temp']], on='Date', how='left')\n",
    "df_1 = df_1.rename(columns={'Temp':'J'})\n",
    "df_1 = pd.merge(df_1, k1[['Date', 'Temp']], on='Date', how='left')\n",
    "df_1 = df_1.rename(columns={'Temp':'K'})\n",
    "df_1 = pd.merge(df_1, m1[['Date', 'Temp']], on='Date', how='left')\n",
    "df_1 = df_1.rename(columns={'Temp':'M'})\n",
    "df_1 = pd.merge(df_1, o1[['Date', 'Temp']], on='Date', how='left')\n",
    "df_1 = df_1.rename(columns={'Temp':'O'})\n",
    "df_1 = pd.merge(df_1, q1[['Date', 'Temp']], on='Date', how='left')\n",
    "df_1 = df_1.rename(columns={'Temp':'Q'})\n",
    "df_1 = pd.merge(df_1, t1[['Date', 'Temp']], on='Date', how='left')\n",
    "df_1 = df_1.rename(columns={'Temp':'T'})\n",
    "df_1 = pd.merge(df_1, prox[['Date', 'wTemp', 'wRH', 'ID']], on='Date', how='left')\n",
    "df_1\n",
    "\n",
    "df_1['method10c']= 0.07 * df_1['T'] + 0.13 * df_1['Q'] + 0.19 * df_1['O'] + 0.12 * df_1['M'] + 0.12 * df_1['K'] + 0.12 * df_1['J'] + 0.05 * df_1['H'] + 0.06 * df_1['F'] + 0.08 * df_1['D'] + 0.06 * df_1['A']\n",
    "df_1.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwI7QFY3-FpQ"
   },
   "source": [
    "### <mark> SET 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owqAukcC-FpR"
   },
   "outputs": [],
   "source": [
    "# paths to the data\n",
    "pa2 = glob.glob(\"FE00000069AF1041\" + '*.csv')\n",
    "pd2 = glob.glob(\"9400000069AA4941\" + '*.csv')\n",
    "pf2 = glob.glob(\"C400000069B3DB41\" + '*.csv')\n",
    "ph2 = glob.glob(\"EC00000069A9CD41\" + '*.csv')\n",
    "pj2 = glob.glob(\"C700000069B34D41\" + '*.csv')\n",
    "pk2 = glob.glob(\"7B00000069AA6A41\" + '*.csv')\n",
    "pm2 = glob.glob(\"8C00000069ADD241\" + '*.csv')\n",
    "po2 = glob.glob(\"F200000069B16D41\" + '*.csv')\n",
    "pq2 = glob.glob(\"6000000069AB3141\" + '*.csv')\n",
    "pt2 = glob.glob(\"9E00000069AE7241\" + '*.csv')\n",
    "\n",
    "# read all data needed\n",
    "a2 = pd.read_csv(''.join(pa2))\n",
    "d2 = pd.read_csv(''.join(pd2))\n",
    "f2 = pd.read_csv(''.join(pf2))\n",
    "h2 = pd.read_csv(''.join(ph2))\n",
    "j2 = pd.read_csv(''.join(pj2))\n",
    "k2 = pd.read_csv(''.join(pk2))\n",
    "m2 = pd.read_csv(''.join(pm2))\n",
    "o2 = pd.read_csv(''.join(po2))\n",
    "q2 = pd.read_csv(''.join(pq2))\n",
    "t2 = pd.read_csv(''.join(pt2))\n",
    "\n",
    "date = a2[\"Date\"]\n",
    "\n",
    "# create dataframe with the first (A) iButton date\n",
    "df_2 = pd.DataFrame(index=[date])\n",
    "\n",
    "# \"vlookup\" to fill the temperature data\n",
    "df_2 = pd.merge(df_2, a2[['Date', 'Temp']], on='Date', how='left')\n",
    "df_2 = df_2.rename(columns={'Temp':'A'})\n",
    "df_2 = pd.merge(df_2, d2[['Date', 'Temp']], on='Date', how='left')\n",
    "df_2 = df_2.rename(columns={'Temp':'D'})\n",
    "df_2 = pd.merge(df_2, f2[['Date', 'Temp']], on='Date', how='left')\n",
    "df_2 = df_2.rename(columns={'Temp':'F'})\n",
    "df_2 = pd.merge(df_2, h2[['Date', 'Temp']], on='Date', how='left')\n",
    "df_2 = df_2.rename(columns={'Temp':'H'})\n",
    "df_2 = pd.merge(df_2, j2[['Date', 'Temp']], on='Date', how='left')\n",
    "df_2 = df_2.rename(columns={'Temp':'J'})\n",
    "df_2 = pd.merge(df_2, k2[['Date', 'Temp']], on='Date', how='left')\n",
    "df_2 = df_2.rename(columns={'Temp':'K'})\n",
    "df_2 = pd.merge(df_2, m2[['Date', 'Temp']], on='Date', how='left')\n",
    "df_2 = df_2.rename(columns={'Temp':'M'})\n",
    "df_2 = pd.merge(df_2, o2[['Date', 'Temp']], on='Date', how='left')\n",
    "df_2 = df_2.rename(columns={'Temp':'O'})\n",
    "df_2 = pd.merge(df_2, q2[['Date', 'Temp']], on='Date', how='left')\n",
    "df_2 = df_2.rename(columns={'Temp':'Q'})\n",
    "df_2 = pd.merge(df_2, t2[['Date', 'Temp']], on='Date', how='left')\n",
    "df_2 = df_2.rename(columns={'Temp':'T'})\n",
    "df_2 = pd.merge(df_2, prox[['Date', 'wTemp', 'wRH', 'ID']], on='Date', how='left')\n",
    "df_2\n",
    "\n",
    "df_2['method10c']= 0.07 * df_2['T'] + 0.13 * df_2['Q'] + 0.19 * df_2['O'] + 0.12 * df_2['M'] + 0.12 * df_2['K'] + 0.12 * df_2['J'] + 0.05 * df_2['H'] + 0.06 * df_2['F'] + 0.08 * df_2['D'] + 0.06 * df_2['A']\n",
    "df_2.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_mvP4Bv-FpS"
   },
   "source": [
    "### <mark> SET 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uZxYpZO-FpT"
   },
   "outputs": [],
   "source": [
    "# paths to the data\n",
    "pa3 = glob.glob(\"B600000069B23241\" + '*.csv')\n",
    "pd3 = glob.glob(\"2700000069AD8541\" + '*.csv')\n",
    "pf3 = glob.glob(\"8100000069AE1641\" + '*.csv')\n",
    "ph3 = glob.glob(\"3300000069AC2241\" + '*.csv')\n",
    "#pj3 = glob.glob(\"3400000069B1AE41\" + '*.csv')  # ou 4B00000069B0A841\n",
    "pj3 = glob.glob(\"4B00000069B0A841\" + '*.csv')\n",
    "pk3 = glob.glob(\"E500000069AA2F41\" + '*.csv')\n",
    "pm3 = glob.glob(\"F000000069AF6941\" + '*.csv')\n",
    "po3 = glob.glob(\"3000000069A9C941\" + '*.csv')\n",
    "pq3 = glob.glob(\"DB00000069A9CC41\" + '*.csv')\n",
    "pt3 = glob.glob(\"6600000069B25C41\" + '*.csv')\n",
    "\n",
    "# read all data needed\n",
    "a3 = pd.read_csv(''.join(pa3))\n",
    "d3 = pd.read_csv(''.join(pd3))\n",
    "f3 = pd.read_csv(''.join(pf3))\n",
    "h3 = pd.read_csv(''.join(ph3))\n",
    "j3 = pd.read_csv(''.join(pj3))\n",
    "k3 = pd.read_csv(''.join(pk3))\n",
    "m3 = pd.read_csv(''.join(pm3))\n",
    "o3 = pd.read_csv(''.join(po3))\n",
    "q3 = pd.read_csv(''.join(pq3))\n",
    "t3 = pd.read_csv(''.join(pt3))\n",
    "\n",
    "date = a3[\"Date\"]\n",
    "\n",
    "# create dataframe with the first (A) iButton date\n",
    "df_3 = pd.DataFrame(index=[date])\n",
    "#df_3\n",
    "\n",
    "# \"vlookup\" to fill the temperature data\n",
    "df_3 = pd.merge(df_3, a3[['Date', 'Temp']], on='Date', how='left')\n",
    "df_3 = df_3.rename(columns={'Temp':'A'})\n",
    "df_3 = pd.merge(df_3, d3[['Date', 'Temp']], on='Date', how='left')\n",
    "df_3 = df_3.rename(columns={'Temp':'D'})\n",
    "df_3 = pd.merge(df_3, f3[['Date', 'Temp']], on='Date', how='left')\n",
    "df_3 = df_3.rename(columns={'Temp':'F'})\n",
    "df_3 = pd.merge(df_3, h3[['Date', 'Temp']], on='Date', how='left')\n",
    "df_3 = df_3.rename(columns={'Temp':'H'})\n",
    "df_3 = pd.merge(df_3, j3[['Date', 'Temp']], on='Date', how='left')\n",
    "df_3 = df_3.rename(columns={'Temp':'J'})\n",
    "df_3 = pd.merge(df_3, k3[['Date', 'Temp']], on='Date', how='left')\n",
    "df_3 = df_3.rename(columns={'Temp':'K'})\n",
    "df_3 = pd.merge(df_3, m3[['Date', 'Temp']], on='Date', how='left')\n",
    "df_3 = df_3.rename(columns={'Temp':'M'})\n",
    "df_3 = pd.merge(df_3, o3[['Date', 'Temp']], on='Date', how='left')\n",
    "df_3 = df_3.rename(columns={'Temp':'O'})\n",
    "df_3 = pd.merge(df_3, q3[['Date', 'Temp']], on='Date', how='left')\n",
    "df_3 = df_3.rename(columns={'Temp':'Q'})\n",
    "df_3 = pd.merge(df_3, t3[['Date', 'Temp']], on='Date', how='left')\n",
    "df_3 = df_3.rename(columns={'Temp':'T'})\n",
    "df_3 = pd.merge(df_3, prox[['Date', 'wTemp', 'wRH', 'ID']], on='Date', how='left')\n",
    "df_3\n",
    "df_3['method10c']= 0.07 * df_3['T'] + 0.13 * df_3['Q'] + 0.19 * df_3['O'] + 0.12 * df_3['M'] + 0.12 * df_3['K'] + 0.12 * df_3['J'] + 0.05 * df_3['H'] + 0.06 * df_3['F'] + 0.08 * df_3['D'] + 0.06 * df_3['A']\n",
    "\n",
    "df_3.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K01MB1xy-FpT"
   },
   "source": [
    "### <mark> SET 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxzceP17-FpT"
   },
   "outputs": [],
   "source": [
    "# paths to the data\n",
    "pa4 = glob.glob(\"7D00000069AB4641\" + '*.csv')\n",
    "pd4 = glob.glob(\"C100000069B26141\" + '*.csv')\n",
    "pf4 = glob.glob(\"2700000069AACA41\" + '*.csv')\n",
    "ph4 = glob.glob(\"3600000069AAD741\" + '*.csv')\n",
    "pj4 = glob.glob(\"0600000069B22741\" + '*.csv')\n",
    "pk4 = glob.glob(\"1A00000069A9BF41\" + '*.csv')\n",
    "pm4 = glob.glob(\"AA00000069AA8141\" + '*.csv')\n",
    "po4 = glob.glob(\"BF00000069AFC741\" + '*.csv')\n",
    "pq4 = glob.glob(\"EF00000069B23141\" + '*.csv')\n",
    "pt4 = glob.glob(\"A200000069B0BE41\" + '*.csv')\n",
    "\n",
    "# read all data needed\n",
    "a4 = pd.read_csv(''.join(pa4))\n",
    "d4 = pd.read_csv(''.join(pd4))\n",
    "f4 = pd.read_csv(''.join(pf4))\n",
    "h4 = pd.read_csv(''.join(ph4))\n",
    "j4 = pd.read_csv(''.join(pj4))\n",
    "k4 = pd.read_csv(''.join(pk4))\n",
    "m4 = pd.read_csv(''.join(pm4))\n",
    "o4 = pd.read_csv(''.join(po4))\n",
    "q4 = pd.read_csv(''.join(pq4))\n",
    "t4 = pd.read_csv(''.join(pt4))\n",
    "\n",
    "date = a4[\"Date\"]\n",
    "\n",
    "# create dataframe with the first (A) iButton date\n",
    "df_4 = pd.DataFrame(index=[date])\n",
    "#df_3\n",
    "\n",
    "# \"vlookup\" to fill the temperature data\n",
    "df_4 = pd.merge(df_4, a4[['Date', 'Temp']], on='Date', how='left')\n",
    "df_4 = df_4.rename(columns={'Temp':'A'})\n",
    "df_4 = pd.merge(df_4, d4[['Date', 'Temp']], on='Date', how='left')\n",
    "df_4 = df_4.rename(columns={'Temp':'D'})\n",
    "df_4 = pd.merge(df_4, f4[['Date', 'Temp']], on='Date', how='left')\n",
    "df_4 = df_4.rename(columns={'Temp':'F'})\n",
    "df_4 = pd.merge(df_4, h4[['Date', 'Temp']], on='Date', how='left')\n",
    "df_4 = df_4.rename(columns={'Temp':'H'})\n",
    "df_4 = pd.merge(df_4, j4[['Date', 'Temp']], on='Date', how='left')\n",
    "df_4 = df_4.rename(columns={'Temp':'J'})\n",
    "df_4 = pd.merge(df_4, k4[['Date', 'Temp']], on='Date', how='left')\n",
    "df_4 = df_4.rename(columns={'Temp':'K'})\n",
    "df_4 = pd.merge(df_4, m4[['Date', 'Temp']], on='Date', how='left')\n",
    "df_4 = df_4.rename(columns={'Temp':'M'})\n",
    "df_4 = pd.merge(df_4, o4[['Date', 'Temp']], on='Date', how='left')\n",
    "df_4 = df_4.rename(columns={'Temp':'O'})\n",
    "df_4 = pd.merge(df_4, q4[['Date', 'Temp']], on='Date', how='left')\n",
    "df_4 = df_4.rename(columns={'Temp':'Q'})\n",
    "df_4 = pd.merge(df_4, t4[['Date', 'Temp']], on='Date', how='left')\n",
    "df_4 = df_4.rename(columns={'Temp':'T'})\n",
    "df_4 = pd.merge(df_4, prox[['Date', 'wTemp', 'wRH', 'ID']], on='Date', how='left')\n",
    "df_4\n",
    "df_4['method10c']= 0.07 * df_4['T'] + 0.13 * df_4['Q'] + 0.19 * df_4['O'] + 0.12 * df_4['M'] + 0.12 * df_4['K'] + 0.12 * df_4['J'] + 0.05 * df_4['H'] + 0.06 * df_4['F'] + 0.08 * df_4['D'] + 0.06 * df_4['A']\n",
    "\n",
    "df_4.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5p-XLDJ-FpU"
   },
   "source": [
    "### <mark> SET 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYpSIRoU-FpU"
   },
   "outputs": [],
   "source": [
    "# paths to the data\n",
    "pa5 = glob.glob(\"0F00000069AAAF41\" + '*.csv')\n",
    "pd5 = glob.glob(\"CA00000069B19041\" + '*.csv')\n",
    "pf5 = glob.glob(\"6700000069AEFC41\" + '*.csv')\n",
    "ph5 = glob.glob(\"E800000069AAC041\" + '*.csv')\n",
    "pj5 = glob.glob(\"9100000069AAEA41\" + '*.csv')\n",
    "pk5 = glob.glob(\"FE00000069B10741\" + '*.csv')\n",
    "pm5 = glob.glob(\"1600000069ABE741\" + '*.csv')\n",
    "po5 = glob.glob(\"A300000069B12241\" + '*.csv')\n",
    "pq5 = glob.glob(\"6900000069AE8541\" + '*.csv')\n",
    "pt5 = glob.glob(\"D700000069B3D541\" + '*.csv')\n",
    "\n",
    "# read all data needed\n",
    "a5 = pd.read_csv(''.join(pa5))\n",
    "d5 = pd.read_csv(''.join(pd5))\n",
    "f5 = pd.read_csv(''.join(pf5))\n",
    "h5 = pd.read_csv(''.join(ph5))\n",
    "j5 = pd.read_csv(''.join(pj5))\n",
    "k5 = pd.read_csv(''.join(pk5))\n",
    "m5 = pd.read_csv(''.join(pm5))\n",
    "o5 = pd.read_csv(''.join(po5))\n",
    "q5 = pd.read_csv(''.join(pq5))\n",
    "t5 = pd.read_csv(''.join(pt5))\n",
    "\n",
    "date = a5[\"Date\"]\n",
    "\n",
    "# create dataframe with the first (A) iButton date\n",
    "df_5 = pd.DataFrame(index=[date])\n",
    "\n",
    "# \"vlookup\" to fill the temperature data\n",
    "df_5 = pd.merge(df_5, a5[['Date', 'Temp']], on='Date', how='left')\n",
    "df_5 = df_5.rename(columns={'Temp':'A'})\n",
    "df_5 = pd.merge(df_5, d5[['Date', 'Temp']], on='Date', how='left')\n",
    "df_5 = df_5.rename(columns={'Temp':'D'})\n",
    "df_5 = pd.merge(df_5, f5[['Date', 'Temp']], on='Date', how='left')\n",
    "df_5 = df_5.rename(columns={'Temp':'F'})\n",
    "df_5 = pd.merge(df_5, h5[['Date', 'Temp']], on='Date', how='left')\n",
    "df_5 = df_5.rename(columns={'Temp':'H'})\n",
    "df_5 = pd.merge(df_5, j5[['Date', 'Temp']], on='Date', how='left')\n",
    "df_5 = df_5.rename(columns={'Temp':'J'})\n",
    "df_5 = pd.merge(df_5, k5[['Date', 'Temp']], on='Date', how='left')\n",
    "df_5 = df_5.rename(columns={'Temp':'K'})\n",
    "df_5 = pd.merge(df_5, m5[['Date', 'Temp']], on='Date', how='left')\n",
    "df_5 = df_5.rename(columns={'Temp':'M'})\n",
    "df_5 = pd.merge(df_5, o5[['Date', 'Temp']], on='Date', how='left')\n",
    "df_5 = df_5.rename(columns={'Temp':'O'})\n",
    "df_5 = pd.merge(df_5, q5[['Date', 'Temp']], on='Date', how='left')\n",
    "df_5 = df_5.rename(columns={'Temp':'Q'})\n",
    "df_5 = pd.merge(df_5, t5[['Date', 'Temp']], on='Date', how='left')\n",
    "df_5 = df_5.rename(columns={'Temp':'T'})\n",
    "df_5 = pd.merge(df_5, prox[['Date', 'wTemp', 'wRH', 'ID']], on='Date', how='left')\n",
    "df_5['method10c']= 0.07 * df_5['T'] + 0.13 * df_5['Q'] + 0.19 * df_5['O'] + 0.12 * df_5['M'] + 0.12 * df_5['K'] + 0.12 * df_5['J'] + 0.05 * df_5['H'] + 0.06 * df_5['F'] + 0.08 * df_5['D'] + 0.06 * df_5['A']\n",
    "\n",
    "df_5.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjxxjk1v-FpU"
   },
   "source": [
    "### <mark> SET 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFtHA8CJ-FpU"
   },
   "outputs": [],
   "source": [
    "# paths to the data\n",
    "pa6 = glob.glob(\"C400000069ADCC41\" + '*.csv')\n",
    "pd6 = glob.glob(\"4A00000069AC0841\" + '*.csv')\n",
    "pf6 = glob.glob(\"1C00000069B3F941\" + '*.csv')\n",
    "ph6 = glob.glob(\"AD00000069AF4C41\" + '*.csv')\n",
    "pj6 = glob.glob(\"6700000069AB8141\" + '*.csv')\n",
    "pk6 = glob.glob(\"E100000069AD4641\" + '*.csv')\n",
    "pm6 = glob.glob(\"A000000069AEBA41\" + '*.csv')\n",
    "po6 = glob.glob(\"D900000069ACA241\" + '*.csv')\n",
    "pq6 = glob.glob(\"4300000069B3CF41\" + '*.csv')\n",
    "pt6 = glob.glob(\"9800000069B26241\" + '*.csv')\n",
    "\n",
    "# read all data needed\n",
    "a6 = pd.read_csv(''.join(pa6))\n",
    "d6 = pd.read_csv(''.join(pd6))\n",
    "f6 = pd.read_csv(''.join(pf6))\n",
    "h6 = pd.read_csv(''.join(ph6))\n",
    "j6 = pd.read_csv(''.join(pj6))\n",
    "k6 = pd.read_csv(''.join(pk6))\n",
    "m6 = pd.read_csv(''.join(pm6))\n",
    "o6 = pd.read_csv(''.join(po6))\n",
    "q6 = pd.read_csv(''.join(pq6))\n",
    "t6 = pd.read_csv(''.join(pt6))\n",
    "\n",
    "date = a6[\"Date\"]\n",
    "\n",
    "# create dataframe with the first (A) iButton date\n",
    "df_6 = pd.DataFrame(index=[date])\n",
    "\n",
    "# \"vlookup\" to fill the temperature data\n",
    "df_6 = pd.merge(df_6, a6[['Date', 'Temp']], on='Date', how='left')\n",
    "df_6 = df_6.rename(columns={'Temp':'A'})\n",
    "df_6 = pd.merge(df_6, d6[['Date', 'Temp']], on='Date', how='left')\n",
    "df_6 = df_6.rename(columns={'Temp':'D'})\n",
    "df_6 = pd.merge(df_6, f6[['Date', 'Temp']], on='Date', how='left')\n",
    "df_6 = df_6.rename(columns={'Temp':'F'})\n",
    "df_6 = pd.merge(df_6, h6[['Date', 'Temp']], on='Date', how='left')\n",
    "df_6 = df_6.rename(columns={'Temp':'H'})\n",
    "df_6 = pd.merge(df_6, j6[['Date', 'Temp']], on='Date', how='left')\n",
    "df_6 = df_6.rename(columns={'Temp':'J'})\n",
    "df_6 = pd.merge(df_6, k6[['Date', 'Temp']], on='Date', how='left')\n",
    "df_6 = df_6.rename(columns={'Temp':'K'})\n",
    "df_6 = pd.merge(df_6, m6[['Date', 'Temp']], on='Date', how='left')\n",
    "df_6 = df_6.rename(columns={'Temp':'M'})\n",
    "df_6 = pd.merge(df_6, o6[['Date', 'Temp']], on='Date', how='left')\n",
    "df_6 = df_6.rename(columns={'Temp':'O'})\n",
    "df_6 = pd.merge(df_6, q6[['Date', 'Temp']], on='Date', how='left')\n",
    "df_6 = df_6.rename(columns={'Temp':'Q'})\n",
    "df_6 = pd.merge(df_6, t6[['Date', 'Temp']], on='Date', how='left')\n",
    "df_6 = df_6.rename(columns={'Temp':'T'})\n",
    "df_6 = pd.merge(df_6, prox[['Date', 'wTemp', 'wRH', 'ID']], on='Date', how='left')\n",
    "df_6['method10c']= 0.07 * df_6['T'] + 0.13 * df_6['Q'] + 0.19 * df_6['O'] + 0.12 * df_6['M'] + 0.12 * df_6['K'] + 0.12 * df_6['J'] + 0.05 * df_6['H'] + 0.06 * df_6['F'] + 0.08 * df_6['D'] + 0.06 * df_6['A']\n",
    "\n",
    "df_6.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh346UNw-FpU"
   },
   "source": [
    "### CONCAT EVERY CSV IN THE FOLDER INTO ONE (run this only if you want to analyse the participants collectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pxe93jCg-FpU",
    "outputId": "746c6d9d-d2c5-4bec-83d5-bf903b52f6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Date       A       D       F       H       J       K  \\\n",
      "0     02/03/23 07:50:01  27.644  28.150  27.972  27.617     NaN     NaN   \n",
      "1     02/03/23 07:51:01  27.706  27.828  27.972  27.554     NaN     NaN   \n",
      "2     02/03/23 07:52:01  27.706  27.703  27.972  27.554  28.958  28.624   \n",
      "3     02/03/23 07:53:01  27.706  27.703  28.340  27.554  28.521  28.436   \n",
      "4     02/03/23 07:54:01  27.644  27.640  27.972  27.554  28.208  28.186   \n",
      "...                 ...     ...     ...     ...     ...     ...     ...   \n",
      "6316  08/02/23 16:56:01  25.332  25.328  25.223  24.867  25.210  25.000   \n",
      "6317  08/02/23 16:57:01  25.270  25.266  25.160  24.867  25.210  25.000   \n",
      "6318  08/02/23 16:58:01  25.145  25.160  25.160  24.867  25.210  24.875   \n",
      "6319  08/02/23 16:59:01  25.200  24.891  25.160  24.867  25.210  24.812   \n",
      "6320  08/02/23 17:00:01  24.895  24.766  25.980     NaN  25.210  24.750   \n",
      "\n",
      "           M       O       Q       T   wTemp     wRH    ID  \n",
      "0        NaN     NaN  27.820  27.827     NaN     NaN  AM02  \n",
      "1     28.687  28.191  27.758  27.702     NaN     NaN  AM02  \n",
      "2     28.499  28.128  27.758  27.640     NaN     NaN  AM02  \n",
      "3     28.312  28.300  27.758  27.577     NaN     NaN  AM02  \n",
      "4     28.125  27.878  27.758  27.515     NaN     NaN  AM02  \n",
      "...      ...     ...     ...     ...     ...     ...   ...  \n",
      "6316  25.000  25.191  25.258  25.141  26.957  54.500  NG23  \n",
      "6317  25.000  25.129  25.195  25.141  26.707  56.530  NG23  \n",
      "6318  25.000  25.129  25.133  25.141  26.519  56.208  NG23  \n",
      "6319  25.000  25.660  25.700  25.780  26.332  57.135  NG23  \n",
      "6320  24.937     NaN  25.800  25.780  26.190  57.135  NG23  \n",
      "\n",
      "[6321 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your CSV files\n",
    "folder_path = 'C:/Users/Natasha/Desktop/Tskin_analises/iButtons/dados_ok/*.csv'\n",
    "\n",
    "# Use the glob library to get a list of all CSV files in the folder\n",
    "files = glob.glob(folder_path)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate through the list of CSV files and read each one into a DataFrame\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list along the rows (axis=0)\n",
    "result_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# Print the concatenated DataFrame\n",
    "print(result_df)\n",
    "\n",
    "# RENAME IF NEEDED\n",
    "# Save the concatenated DataFrame to a new CSV file if needed\n",
    "result_df.to_csv('CONCAT.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
